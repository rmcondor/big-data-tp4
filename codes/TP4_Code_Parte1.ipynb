{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data - TP #4\n",
    "\n",
    "Integrantes: Ronny M. Condor, Diego Fasan y María Camila Riancho\n",
    "\n",
    "El objetivo de este TP es hacer clasificación y regularización aplicada a la EPH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm     \n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score \n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte I: Análisis de la base de hogares y cálculo de pobreza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importamos la base de datos del primer trimestre del 2023. Conservamos las observaciones de CABA y Gran Buenos Aires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33 32]\n"
     ]
    }
   ],
   "source": [
    "#1) Abrimos el archivo y vemos las primeras cinco filas\n",
    "eph_hogar = pd.read_excel(\"../datasets/usu_hogar_T123.xlsx\")\n",
    "eph_hogar.head(5)\n",
    "\n",
    "# Nos quedamos con las observaciones que pertenecen CABA o Gran Buenos Aires.\n",
    "\n",
    "#Eliminamos las observaciones que no son de CABA (32) ni de Gran Buenos Aires (33).\n",
    "eph_hogar = eph_hogar.drop(eph_hogar[(eph_hogar[\"AGLOMERADO\"] != 32) & (eph_hogar[\"AGLOMERADO\"] != 33)].index)\n",
    "\n",
    "#Para comprobar que funcionó, presentamos los valores que toma la variable \"AGLOMERADO\":\n",
    "print(eph_hogar[\"AGLOMERADO\"].unique() ) #Vemos que ahora \"Aglomerado\" solo toma los valores 32 y 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2736 entries, 9 to 16814\n",
      "Data columns (total 88 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   CODUSU      2736 non-null   object \n",
      " 1   ANO4        2736 non-null   int64  \n",
      " 2   TRIMESTRE   2736 non-null   int64  \n",
      " 3   NRO_HOGAR   2736 non-null   int64  \n",
      " 4   REALIZADA   2736 non-null   int64  \n",
      " 5   REGION      2736 non-null   int64  \n",
      " 6   MAS_500     2736 non-null   object \n",
      " 7   AGLOMERADO  2736 non-null   int64  \n",
      " 8   PONDERA     2736 non-null   int64  \n",
      " 9   IV1         2736 non-null   int64  \n",
      " 10  IV1_ESP     5 non-null      object \n",
      " 11  IV2         2736 non-null   int64  \n",
      " 12  IV3         2736 non-null   int64  \n",
      " 13  IV3_ESP     5 non-null      object \n",
      " 14  IV4         2736 non-null   int64  \n",
      " 15  IV5         2736 non-null   int64  \n",
      " 16  IV6         2736 non-null   int64  \n",
      " 17  IV7         2736 non-null   int64  \n",
      " 18  IV7_ESP     4 non-null      object \n",
      " 19  IV8         2736 non-null   int64  \n",
      " 20  IV9         2736 non-null   int64  \n",
      " 21  IV10        2736 non-null   int64  \n",
      " 22  IV11        2736 non-null   int64  \n",
      " 23  IV12_1      2736 non-null   int64  \n",
      " 24  IV12_2      2736 non-null   int64  \n",
      " 25  IV12_3      2736 non-null   int64  \n",
      " 26  II1         2736 non-null   int64  \n",
      " 27  II2         2736 non-null   int64  \n",
      " 28  II3         2736 non-null   int64  \n",
      " 29  II3_1       2736 non-null   int64  \n",
      " 30  II4_1       2736 non-null   int64  \n",
      " 31  II4_2       2736 non-null   int64  \n",
      " 32  II4_3       2736 non-null   int64  \n",
      " 33  II5         2736 non-null   int64  \n",
      " 34  II5_1       2736 non-null   int64  \n",
      " 35  II6         2736 non-null   int64  \n",
      " 36  II6_1       2736 non-null   int64  \n",
      " 37  II7         2736 non-null   int64  \n",
      " 38  II7_ESP     13 non-null     object \n",
      " 39  II8         2736 non-null   int64  \n",
      " 40  II8_ESP     106 non-null    object \n",
      " 41  II9         2736 non-null   int64  \n",
      " 42  V1          2736 non-null   int64  \n",
      " 43  V2          2736 non-null   int64  \n",
      " 44  V21         2736 non-null   int64  \n",
      " 45  V22         2736 non-null   int64  \n",
      " 46  V3          2736 non-null   int64  \n",
      " 47  V4          2736 non-null   int64  \n",
      " 48  V5          2736 non-null   int64  \n",
      " 49  V6          2736 non-null   int64  \n",
      " 50  V7          2736 non-null   int64  \n",
      " 51  V8          2736 non-null   int64  \n",
      " 52  V9          2736 non-null   int64  \n",
      " 53  V10         2736 non-null   int64  \n",
      " 54  V11         2736 non-null   int64  \n",
      " 55  V12         2736 non-null   int64  \n",
      " 56  V13         2736 non-null   int64  \n",
      " 57  V14         2736 non-null   int64  \n",
      " 58  V15         2736 non-null   int64  \n",
      " 59  V16         2736 non-null   int64  \n",
      " 60  V17         2736 non-null   int64  \n",
      " 61  V18         2736 non-null   int64  \n",
      " 62  V19_A       2736 non-null   int64  \n",
      " 63  V19_B       2736 non-null   int64  \n",
      " 64  IX_TOT      2736 non-null   int64  \n",
      " 65  IX_MEN10    2736 non-null   int64  \n",
      " 66  IX_MAYEQ10  2736 non-null   int64  \n",
      " 67  ITF         2736 non-null   int64  \n",
      " 68  DECIFR      2736 non-null   int64  \n",
      " 69  IDECIFR     0 non-null      float64\n",
      " 70  RDECIFR     2736 non-null   int64  \n",
      " 71  GDECIFR     2736 non-null   float64\n",
      " 72  PDECIFR     0 non-null      float64\n",
      " 73  ADECIFR     2736 non-null   int64  \n",
      " 74  IPCF        2736 non-null   float64\n",
      " 75  DECCFR      2736 non-null   int64  \n",
      " 76  IDECCFR     0 non-null      float64\n",
      " 77  RDECCFR     2736 non-null   int64  \n",
      " 78  GDECCFR     2736 non-null   float64\n",
      " 79  PDECCFR     0 non-null      float64\n",
      " 80  ADECCFR     2736 non-null   int64  \n",
      " 81  PONDIH      2736 non-null   int64  \n",
      " 82  VII1_1      2736 non-null   int64  \n",
      " 83  VII1_2      2736 non-null   int64  \n",
      " 84  VII2_1      2736 non-null   int64  \n",
      " 85  VII2_2      2736 non-null   int64  \n",
      " 86  VII2_3      2736 non-null   int64  \n",
      " 87  VII2_4      2736 non-null   int64  \n",
      "dtypes: float64(7), int64(74), object(7)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "#Resumen de la base de datos\n",
    "eph_hogar.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ahora uniremos la base a nivel hogar con la base a nivel individual, usando las variables llaves `CODUSU` y `NRO_HOGAR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODUSU</th>\n",
       "      <th>ANO4</th>\n",
       "      <th>TRIMESTRE</th>\n",
       "      <th>NRO_HOGAR</th>\n",
       "      <th>COMPONENTE</th>\n",
       "      <th>H15</th>\n",
       "      <th>REGION</th>\n",
       "      <th>MAS_500</th>\n",
       "      <th>AGLOMERADO</th>\n",
       "      <th>PONDERA</th>\n",
       "      <th>...</th>\n",
       "      <th>PDECIFR</th>\n",
       "      <th>ADECIFR</th>\n",
       "      <th>IPCF</th>\n",
       "      <th>DECCFR</th>\n",
       "      <th>IDECCFR</th>\n",
       "      <th>RDECCFR</th>\n",
       "      <th>GDECCFR</th>\n",
       "      <th>PDECCFR</th>\n",
       "      <th>ADECCFR</th>\n",
       "      <th>PONDIH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TQRMNORVWHLMKOCDEOHCH00720228</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>N</td>\n",
       "      <td>91</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9</td>\n",
       "      <td>79700.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TQRMNOPSTHKMKPCDEOHCH00781447</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>N</td>\n",
       "      <td>91</td>\n",
       "      <td>190</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TQRMNOQSXHMOKRCDEOHCH00803177</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>N</td>\n",
       "      <td>91</td>\n",
       "      <td>134</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>145000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TQRMNOQSXHMOKRCDEOHCH00803177</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>N</td>\n",
       "      <td>91</td>\n",
       "      <td>134</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>145000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TQRMNOQYTHMNKSCDEOHCH00803178</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>N</td>\n",
       "      <td>91</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          CODUSU  ANO4  TRIMESTRE  NRO_HOGAR  COMPONENTE  H15  \\\n",
       "0  TQRMNORVWHLMKOCDEOHCH00720228  2023          1          1           5    0   \n",
       "1  TQRMNOPSTHKMKPCDEOHCH00781447  2023          1          1           1    1   \n",
       "2  TQRMNOQSXHMOKRCDEOHCH00803177  2023          1          1           1    1   \n",
       "3  TQRMNOQSXHMOKRCDEOHCH00803177  2023          1          1           2    1   \n",
       "4  TQRMNOQYTHMNKSCDEOHCH00803178  2023          1          1           1    1   \n",
       "\n",
       "   REGION MAS_500  AGLOMERADO  PONDERA  ...  PDECIFR  ADECIFR      IPCF  \\\n",
       "0      44       N          91      112  ...     10.0        9   79700.0   \n",
       "1      44       N          91      190  ...      6.0        5  180000.0   \n",
       "2      44       N          91      134  ...      8.0        8  145000.0   \n",
       "3      44       N          91      134  ...      8.0        8  145000.0   \n",
       "4      44       N          91      120  ...     12.0       12       0.0   \n",
       "\n",
       "   DECCFR  IDECCFR  RDECCFR  GDECCFR  PDECCFR  ADECCFR  PONDIH  \n",
       "0       6      6.0        5      NaN      7.0        5     133  \n",
       "1       9     10.0        9      NaN     10.0        9     200  \n",
       "2       9      9.0        8      NaN      9.0        8     140  \n",
       "3       9      9.0        8      NaN      9.0        8     140  \n",
       "4      12     12.0       12      NaN     12.0       12       0  \n",
       "\n",
       "[5 rows x 177 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Abrimos la base individual\n",
    "eph_indiv = pd.read_excel(\"../datasets/usu_individual_T123.xlsx\")\n",
    "eph_indiv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unimos las bases de tal manera de quedarnos con las observaciones a nivel individuo, sumando las variables a nivel hogar presentes en la encuesta de hogar:\n",
    "\n",
    "eph=eph_indiv.merge(eph_hogar, on=['CODUSU','NRO_HOGAR'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 48638 entries, 0 to 48637\n",
      "Data columns (total 263 columns):\n",
      " #    Column        Dtype  \n",
      "---   ------        -----  \n",
      " 0    CODUSU        object \n",
      " 1    ANO4_x        int64  \n",
      " 2    TRIMESTRE_x   int64  \n",
      " 3    NRO_HOGAR     int64  \n",
      " 4    COMPONENTE    int64  \n",
      " 5    H15           int64  \n",
      " 6    REGION_x      int64  \n",
      " 7    MAS_500_x     object \n",
      " 8    AGLOMERADO_x  int64  \n",
      " 9    PONDERA_x     int64  \n",
      " 10   CH03          int64  \n",
      " 11   CH04          int64  \n",
      " 12   CH05          object \n",
      " 13   CH06          int64  \n",
      " 14   CH07          int64  \n",
      " 15   CH08          float64\n",
      " 16   CH09          int64  \n",
      " 17   CH10          int64  \n",
      " 18   CH11          int64  \n",
      " 19   CH12          int64  \n",
      " 20   CH13          int64  \n",
      " 21   CH14          float64\n",
      " 22   CH15          int64  \n",
      " 23   CH15_COD      float64\n",
      " 24   CH16          int64  \n",
      " 25   CH16_COD      float64\n",
      " 26   NIVEL_ED      int64  \n",
      " 27   ESTADO        int64  \n",
      " 28   CAT_OCUP      int64  \n",
      " 29   CAT_INAC      int64  \n",
      " 30   IMPUTA        float64\n",
      " 31   PP02C1        int64  \n",
      " 32   PP02C2        int64  \n",
      " 33   PP02C3        int64  \n",
      " 34   PP02C4        int64  \n",
      " 35   PP02C5        int64  \n",
      " 36   PP02C6        int64  \n",
      " 37   PP02C7        int64  \n",
      " 38   PP02C8        int64  \n",
      " 39   PP02E         int64  \n",
      " 40   PP02H         int64  \n",
      " 41   PP02I         int64  \n",
      " 42   PP03C         float64\n",
      " 43   PP03D         float64\n",
      " 44   PP3E_TOT      float64\n",
      " 45   PP3F_TOT      float64\n",
      " 46   PP03G         float64\n",
      " 47   PP03H         float64\n",
      " 48   PP03I         float64\n",
      " 49   PP03J         float64\n",
      " 50   INTENSI       float64\n",
      " 51   PP04A         float64\n",
      " 52   PP04B_COD     float64\n",
      " 53   PP04B1        float64\n",
      " 54   PP04B2        float64\n",
      " 55   PP04B3_MES    float64\n",
      " 56   PP04B3_ANO    float64\n",
      " 57   PP04B3_DIA    float64\n",
      " 58   PP04C         float64\n",
      " 59   PP04C99       float64\n",
      " 60   PP04D_COD     float64\n",
      " 61   PP04G         float64\n",
      " 62   PP05B2_MES    float64\n",
      " 63   PP05B2_ANO    float64\n",
      " 64   PP05B2_DIA    float64\n",
      " 65   PP05C_1       float64\n",
      " 66   PP05C_2       float64\n",
      " 67   PP05C_3       float64\n",
      " 68   PP05E         float64\n",
      " 69   PP05F         float64\n",
      " 70   PP05H         float64\n",
      " 71   PP06A         float64\n",
      " 72   PP06C         float64\n",
      " 73   PP06D         float64\n",
      " 74   PP06E         float64\n",
      " 75   PP06H         float64\n",
      " 76   PP07A         float64\n",
      " 77   PP07C         float64\n",
      " 78   PP07D         float64\n",
      " 79   PP07E         float64\n",
      " 80   PP07F1        float64\n",
      " 81   PP07F2        float64\n",
      " 82   PP07F3        float64\n",
      " 83   PP07F4        float64\n",
      " 84   PP07F5        float64\n",
      " 85   PP07G1        float64\n",
      " 86   PP07G2        float64\n",
      " 87   PP07G3        float64\n",
      " 88   PP07G4        float64\n",
      " 89   PP07G_59      float64\n",
      " 90   PP07H         float64\n",
      " 91   PP07I         float64\n",
      " 92   PP07J         float64\n",
      " 93   PP07K         float64\n",
      " 94   PP08D1        float64\n",
      " 95   PP08D4        float64\n",
      " 96   PP08F1        float64\n",
      " 97   PP08F2        float64\n",
      " 98   PP08J1        float64\n",
      " 99   PP08J2        float64\n",
      " 100  PP08J3        float64\n",
      " 101  PP09A         float64\n",
      " 102  PP09A_ESP     object \n",
      " 103  PP09B         float64\n",
      " 104  PP09C         float64\n",
      " 105  PP09C_ESP     object \n",
      " 106  PP10A         float64\n",
      " 107  PP10C         float64\n",
      " 108  PP10D         float64\n",
      " 109  PP10E         float64\n",
      " 110  PP11A         float64\n",
      " 111  PP11B_COD     float64\n",
      " 112  PP11B1        float64\n",
      " 113  PP11B2_MES    float64\n",
      " 114  PP11B2_ANO    float64\n",
      " 115  PP11B2_DIA    float64\n",
      " 116  PP11C         float64\n",
      " 117  PP11C99       float64\n",
      " 118  PP11D_COD     float64\n",
      " 119  PP11G_ANO     float64\n",
      " 120  PP11G_MES     float64\n",
      " 121  PP11G_DIA     float64\n",
      " 122  PP11L         float64\n",
      " 123  PP11L1        float64\n",
      " 124  PP11M         float64\n",
      " 125  PP11N         float64\n",
      " 126  PP11O         float64\n",
      " 127  PP11P         float64\n",
      " 128  PP11Q         float64\n",
      " 129  PP11R         float64\n",
      " 130  PP11S         float64\n",
      " 131  PP11T         float64\n",
      " 132  P21           int64  \n",
      " 133  DECOCUR       int64  \n",
      " 134  IDECOCUR      float64\n",
      " 135  RDECOCUR      int64  \n",
      " 136  GDECOCUR      float64\n",
      " 137  PDECOCUR      float64\n",
      " 138  ADECOCUR      int64  \n",
      " 139  PONDIIO       int64  \n",
      " 140  TOT_P12       int64  \n",
      " 141  P47T          float64\n",
      " 142  DECINDR       int64  \n",
      " 143  IDECINDR      float64\n",
      " 144  RDECINDR      int64  \n",
      " 145  GDECINDR      float64\n",
      " 146  PDECINDR      float64\n",
      " 147  ADECINDR      int64  \n",
      " 148  PONDII        int64  \n",
      " 149  V2_M          int64  \n",
      " 150  V3_M          int64  \n",
      " 151  V4_M          int64  \n",
      " 152  V5_M          int64  \n",
      " 153  V8_M          int64  \n",
      " 154  V9_M          int64  \n",
      " 155  V10_M         int64  \n",
      " 156  V11_M         int64  \n",
      " 157  V12_M         int64  \n",
      " 158  V18_M         int64  \n",
      " 159  V19_AM        int64  \n",
      " 160  V21_M         int64  \n",
      " 161  T_VI          int64  \n",
      " 162  ITF_x         int64  \n",
      " 163  DECIFR_x      int64  \n",
      " 164  IDECIFR_x     float64\n",
      " 165  RDECIFR_x     int64  \n",
      " 166  GDECIFR_x     float64\n",
      " 167  PDECIFR_x     float64\n",
      " 168  ADECIFR_x     int64  \n",
      " 169  IPCF_x        float64\n",
      " 170  DECCFR_x      int64  \n",
      " 171  IDECCFR_x     float64\n",
      " 172  RDECCFR_x     int64  \n",
      " 173  GDECCFR_x     float64\n",
      " 174  PDECCFR_x     float64\n",
      " 175  ADECCFR_x     int64  \n",
      " 176  PONDIH_x      int64  \n",
      " 177  ANO4_y        float64\n",
      " 178  TRIMESTRE_y   float64\n",
      " 179  REALIZADA     float64\n",
      " 180  REGION_y      float64\n",
      " 181  MAS_500_y     object \n",
      " 182  AGLOMERADO_y  float64\n",
      " 183  PONDERA_y     float64\n",
      " 184  IV1           float64\n",
      " 185  IV1_ESP       object \n",
      " 186  IV2           float64\n",
      " 187  IV3           float64\n",
      " 188  IV3_ESP       object \n",
      " 189  IV4           float64\n",
      " 190  IV5           float64\n",
      " 191  IV6           float64\n",
      " 192  IV7           float64\n",
      " 193  IV7_ESP       object \n",
      " 194  IV8           float64\n",
      " 195  IV9           float64\n",
      " 196  IV10          float64\n",
      " 197  IV11          float64\n",
      " 198  IV12_1        float64\n",
      " 199  IV12_2        float64\n",
      " 200  IV12_3        float64\n",
      " 201  II1           float64\n",
      " 202  II2           float64\n",
      " 203  II3           float64\n",
      " 204  II3_1         float64\n",
      " 205  II4_1         float64\n",
      " 206  II4_2         float64\n",
      " 207  II4_3         float64\n",
      " 208  II5           float64\n",
      " 209  II5_1         float64\n",
      " 210  II6           float64\n",
      " 211  II6_1         float64\n",
      " 212  II7           float64\n",
      " 213  II7_ESP       object \n",
      " 214  II8           float64\n",
      " 215  II8_ESP       object \n",
      " 216  II9           float64\n",
      " 217  V1            float64\n",
      " 218  V2            float64\n",
      " 219  V21           float64\n",
      " 220  V22           float64\n",
      " 221  V3            float64\n",
      " 222  V4            float64\n",
      " 223  V5            float64\n",
      " 224  V6            float64\n",
      " 225  V7            float64\n",
      " 226  V8            float64\n",
      " 227  V9            float64\n",
      " 228  V10           float64\n",
      " 229  V11           float64\n",
      " 230  V12           float64\n",
      " 231  V13           float64\n",
      " 232  V14           float64\n",
      " 233  V15           float64\n",
      " 234  V16           float64\n",
      " 235  V17           float64\n",
      " 236  V18           float64\n",
      " 237  V19_A         float64\n",
      " 238  V19_B         float64\n",
      " 239  IX_TOT        float64\n",
      " 240  IX_MEN10      float64\n",
      " 241  IX_MAYEQ10    float64\n",
      " 242  ITF_y         float64\n",
      " 243  DECIFR_y      float64\n",
      " 244  IDECIFR_y     float64\n",
      " 245  RDECIFR_y     float64\n",
      " 246  GDECIFR_y     float64\n",
      " 247  PDECIFR_y     float64\n",
      " 248  ADECIFR_y     float64\n",
      " 249  IPCF_y        float64\n",
      " 250  DECCFR_y      float64\n",
      " 251  IDECCFR_y     float64\n",
      " 252  RDECCFR_y     float64\n",
      " 253  GDECCFR_y     float64\n",
      " 254  PDECCFR_y     float64\n",
      " 255  ADECCFR_y     float64\n",
      " 256  PONDIH_y      float64\n",
      " 257  VII1_1        float64\n",
      " 258  VII1_2        float64\n",
      " 259  VII2_1        float64\n",
      " 260  VII2_2        float64\n",
      " 261  VII2_3        float64\n",
      " 262  VII2_4        float64\n",
      "dtypes: float64(187), int64(65), object(11)\n",
      "memory usage: 98.0+ MB\n"
     ]
    }
   ],
   "source": [
    "#Resumen de la base de datos\n",
    "eph.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Para la limpieza de datos, usaremos distintas funciones de los paquetes `numpy` y `pandas`. A continuación, comentaremos el uso que le dimos a las funciones:\n",
    "\n",
    "* `drop`: Elimina filas o columnas de un dataframe. La usaremos para borrar a las observaciones que no pertenecen a CABA o Gran Buenos Aires. Además, nos ayudará a borrar observaciones que no tienen sentido, como por ejemplo, edades negativas.\n",
    "* `replace`: Reemplaza valores en un dataframe. Nos ayudará a reemplazar valores de *no respuesta* (por ejemplo, 9, 99, 999) por missing values \"nan\".\n",
    "* `np.nan`: Representa el valor \"nan\" el cual usamos para reemplazar en las observaciones que decidamos deberán llevar un missing.\n",
    "* `np.nanmin` y `np.nanmax`: Calculan el mínimo y máximo, excluyendo valores nulos en NumPy. Lo usaremos también para reemplazar los valores de *no respuesta* por missing values. Específicamente, nos ayudará a buscar por todos los valores no missing de cada variable que puede tomar un valor de *no respuesta*.\n",
    "* `dropna`:La usaremos para borrar las columnas que tienen solo missings values. Esto es necesario puesto que cuando corramos los modelos es necesario tener la data sin missing values.\n",
    "* `pd.get_dummies`: Como tenemos variables categóricas que, en algunos casos, no están ordenadas, planteamos crear dummies para cada categoría. Con esta función, logramos este objetivo y obtenemos una mayor cantidad de variables.\n",
    "* `pd.concat`: Combina dataframes a lo largo de filas o columnas. Lo usamos para concatenar las variables completamente numéricas y las dummies que creamos. De esta manera tenemos el dataframe final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos variables duplicadas:\n",
    "# Primero eliminamos los sufijos para que las columnas que son iguales tengan el mismo nombre\n",
    "variables=eph.columns.tolist()\n",
    "nombres_columnas=[]\n",
    "for v in variables:\n",
    "    nombres_columnas.append(v.replace(\"_x\", \"\").replace(\"_y\", \"\"))\n",
    "eph.columns=nombres_columnas  \n",
    "# Eliminamos las columnas duplicadas:\n",
    "eph = eph.loc[:, ~eph.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODUSU TQRMNOPPQHJKLLCDEFKID00798130 TQVMNOVXQHMOMOCDEFKID00798676\n",
      "ANO4 2023 2023\n",
      "TRIMESTRE 1 1\n",
      "NRO_HOGAR 1 52\n",
      "COMPONENTE 1 16\n",
      "H15 0 2\n",
      "REGION 1 44\n",
      "MAS_500 N S\n",
      "AGLOMERADO 2 93\n",
      "PONDERA 24 8423\n",
      "CH03 1 10\n",
      "CH04 1 2\n",
      "CH05 1900-01-01 00:00:00 9999-09-09 00:00:00\n",
      "CH06 -1 101\n",
      "CH07 1 9\n",
      "CH08 1.0 23.0\n",
      "CH09 0 9\n",
      "CH10 0 9\n",
      "CH11 0 9\n",
      "CH12 0 99\n",
      "CH13 0 9\n",
      "CH14 0.0 99.0\n",
      "CH15 0 9\n",
      "CH15_COD 2.0 999.0\n",
      "CH16 0 9\n",
      "CH16_COD 2.0 498.0\n",
      "NIVEL_ED 1 7\n",
      "ESTADO 0 4\n",
      "CAT_OCUP 0 9\n",
      "CAT_INAC 0 7\n",
      "IMPUTA 1.0 1.0\n",
      "PP02C1 0 2\n",
      "PP02C2 0 2\n",
      "PP02C3 0 2\n",
      "PP02C4 0 2\n",
      "PP02C5 0 2\n",
      "PP02C6 0 2\n",
      "PP02C7 0 2\n",
      "PP02C8 0 2\n",
      "PP02E 0 5\n",
      "PP02H 0 2\n",
      "PP02I 0 2\n",
      "PP03C 0.0 2.0\n",
      "PP03D 0.0 9.0\n",
      "PP3E_TOT 0.0 999.0\n",
      "PP3F_TOT 0.0 999.0\n",
      "PP03G 1.0 9.0\n",
      "PP03H 0.0 9.0\n",
      "PP03I 1.0 9.0\n",
      "PP03J 1.0 9.0\n",
      "INTENSI 1.0 4.0\n",
      "PP04A 1.0 9.0\n",
      "PP04B_COD 1.0 9999.0\n",
      "PP04B1 1.0 2.0\n",
      "PP04B2 0.0 10.0\n",
      "PP04B3_MES 0.0 99.0\n",
      "PP04B3_ANO 0.0 99.0\n",
      "PP04B3_DIA 0.0 99.0\n",
      "PP04C 0.0 99.0\n",
      "PP04C99 0.0 9.0\n",
      "PP04D_COD 1.0 99999.0\n",
      "PP04G 0.0 99.0\n",
      "PP05B2_MES 0.0 99.0\n",
      "PP05B2_ANO 0.0 99.0\n",
      "PP05B2_DIA 0.0 99.0\n",
      "PP05C_1 0.0 9.0\n",
      "PP05C_2 0.0 9.0\n",
      "PP05C_3 0.0 9.0\n",
      "PP05E 0.0 9.0\n",
      "PP05F 0.0 9.0\n",
      "PP05H 0.0 9.0\n",
      "PP06A 0.0 9.0\n",
      "PP06C -9.0 3000000.0\n",
      "PP06D -9.0 2000000.0\n",
      "PP06E 0.0 9.0\n",
      "PP06H 0.0 2.0\n",
      "PP07A 0.0 9.0\n",
      "PP07C 0.0 9.0\n",
      "PP07D 0.0 9.0\n",
      "PP07E 0.0 9.0\n",
      "PP07F1 0.0 9.0\n",
      "PP07F2 0.0 9.0\n",
      "PP07F3 0.0 9.0\n",
      "PP07F4 0.0 9.0\n",
      "PP07F5 0.0 5.0\n",
      "PP07G1 0.0 9.0\n",
      "PP07G2 0.0 9.0\n",
      "PP07G3 0.0 9.0\n",
      "PP07G4 0.0 9.0\n",
      "PP07G_59 0.0 5.0\n",
      "PP07H 0.0 2.0\n",
      "PP07I 0.0 9.0\n",
      "PP07J 0.0 9.0\n",
      "PP07K 0.0 9.0\n",
      "PP08D1 -9.0 4000000.0\n",
      "PP08D4 -9.0 40000.0\n",
      "PP08F1 -9.0 400000.0\n",
      "PP08F2 -9.0 96000.0\n",
      "PP08J1 -9.0 1000000.0\n",
      "PP08J2 -9.0 800000.0\n",
      "PP08J3 0.0 50000.0\n",
      "PP09A 0.0 9.0\n",
      "PP09A_ESP\n",
      "PP09B 0.0 2.0\n",
      "PP09C 0.0 2.0\n",
      "PP09C_ESP\n",
      "PP10A 1.0 9.0\n",
      "PP10C 1.0 2.0\n",
      "PP10D 0.0 2.0\n",
      "PP10E 0.0 6.0\n",
      "PP11A 0.0 9.0\n",
      "PP11B_COD 0.0 9999.0\n",
      "PP11B1 0.0 9.0\n",
      "PP11B2_MES 0.0 10.0\n",
      "PP11B2_ANO 0.0 14.0\n",
      "PP11B2_DIA 0.0 30.0\n",
      "PP11C 0.0 99.0\n",
      "PP11C99 0.0 9.0\n",
      "PP11D_COD 5002.0 99999.0\n",
      "PP11G_ANO 0.0 99.0\n",
      "PP11G_MES 0.0 99.0\n",
      "PP11G_DIA 0.0 99.0\n",
      "PP11L 0.0 9.0\n",
      "PP11L1 0.0 3.0\n",
      "PP11M 0.0 3.0\n",
      "PP11N 0.0 9.0\n",
      "PP11O 0.0 9.0\n",
      "PP11P 0.0 9.0\n",
      "PP11Q 0.0 9.0\n",
      "PP11R 0.0 2.0\n",
      "PP11S 0.0 2.0\n",
      "PP11T 0.0 9.0\n",
      "P21 -9 4000000\n",
      "DECOCUR 0 12\n",
      "IDECOCUR 0.0 12.0\n",
      "RDECOCUR 0 12\n",
      "GDECOCUR 0.0 12.0\n",
      "PDECOCUR 0.0 12.0\n",
      "ADECOCUR 0 12\n",
      "PONDIIO 0 16574\n",
      "TOT_P12 -9 6500000\n",
      "P47T -9.0 10750000.0\n",
      "DECINDR 0 13\n",
      "IDECINDR 0.0 13.0\n",
      "RDECINDR 0 13\n",
      "GDECINDR 0.0 13.0\n",
      "PDECINDR 0.0 13.0\n",
      "ADECINDR 0 13\n",
      "PONDII 0 15735\n",
      "V2_M -9 1080000\n",
      "V3_M -9 1000000\n",
      "V4_M -9 160000\n",
      "V5_M -9 149000\n",
      "V8_M -9 3800000\n",
      "V9_M -9 400000\n",
      "V10_M -9 500000\n",
      "V11_M -9 70000\n",
      "V12_M -9 400000\n",
      "V18_M -9 500000\n",
      "V19_AM 0 0\n",
      "V21_M -9 540000\n",
      "T_VI -9 3800000\n",
      "ITF 0 10990000\n",
      "DECIFR 0 12\n",
      "IDECIFR 0.0 12.0\n",
      "RDECIFR 0 12\n",
      "GDECIFR 0.0 12.0\n",
      "PDECIFR 0.0 12.0\n",
      "ADECIFR 0 12\n",
      "IPCF 0.0 3000000.0\n",
      "DECCFR 0 12\n",
      "IDECCFR 0.0 12.0\n",
      "RDECCFR 0 12\n",
      "GDECCFR 0.0 12.0\n",
      "PDECCFR 0.0 12.0\n",
      "ADECCFR 0 12\n",
      "PONDIH 0 20663\n",
      "REALIZADA 1.0 1.0\n",
      "IV1 1.0 6.0\n",
      "IV1_ESP\n",
      "IV2 1.0 99.0\n",
      "IV3 1.0 4.0\n",
      "IV3_ESP\n",
      "IV4 1.0 9.0\n",
      "IV5 1.0 2.0\n",
      "IV6 1.0 3.0\n",
      "IV7 1.0 4.0\n",
      "IV7_ESP\n",
      "IV8 1.0 2.0\n",
      "IV9 0.0 2.0\n",
      "IV10 0.0 3.0\n",
      "IV11 0.0 4.0\n",
      "IV12_1 1.0 2.0\n",
      "IV12_2 1.0 2.0\n",
      "IV12_3 1.0 2.0\n",
      "II1 0.0 99.0\n",
      "II2 0.0 5.0\n",
      "II3 0.0 2.0\n",
      "II3_1 0.0 3.0\n",
      "II4_1 0.0 2.0\n",
      "II4_2 0.0 2.0\n",
      "II4_3 0.0 2.0\n",
      "II5 0.0 2.0\n",
      "II5_1 0.0 2.0\n",
      "II6 0.0 2.0\n",
      "II6_1 0.0 2.0\n",
      "II7 0.0 9.0\n",
      "II7_ESP\n",
      "II8 0.0 4.0\n",
      "II8_ESP\n",
      "II9 0.0 4.0\n",
      "V1 1.0 9.0\n",
      "V2 1.0 9.0\n",
      "V21 1.0 2.0\n",
      "V22 1.0 2.0\n",
      "V3 1.0 9.0\n",
      "V4 1.0 9.0\n",
      "V5 1.0 9.0\n",
      "V6 1.0 9.0\n",
      "V7 1.0 9.0\n",
      "V8 1.0 9.0\n",
      "V9 1.0 9.0\n",
      "V10 1.0 9.0\n",
      "V11 1.0 9.0\n",
      "V12 1.0 9.0\n",
      "V13 1.0 9.0\n",
      "V14 1.0 9.0\n",
      "V15 1.0 9.0\n",
      "V16 1.0 9.0\n",
      "V17 1.0 9.0\n",
      "V18 1.0 9.0\n",
      "V19_A 2.0 9.0\n",
      "V19_B 2.0 9.0\n",
      "IX_TOT 1.0 13.0\n",
      "IX_MEN10 0.0 8.0\n",
      "IX_MAYEQ10 1.0 10.0\n",
      "VII1_1 1.0 99.0\n",
      "VII1_2 0.0 97.0\n",
      "VII2_1 1.0 98.0\n",
      "VII2_2 0.0 98.0\n",
      "VII2_3 0.0 98.0\n",
      "VII2_4 0.0 6.0\n"
     ]
    }
   ],
   "source": [
    "# Para identificar aquellas variables que toman valores sin sentido, utilizamos el siguiente comando:\n",
    "# (Las funciones nanmin y nanmax calculan los valores mínimos y máximos de cada columna sin incluir los valores faltantes, ya que de lo contrario para las variables con valores faltantes \"nan\" aparece como el valor mínimo y máximo)\n",
    "\n",
    "for column in eph.columns:\n",
    "    try:\n",
    "        print(column, np.nanmin(eph[column]), np.nanmax(eph[column]))\n",
    "    except:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP09A_ESP object\n",
      "PP09C_ESP object\n",
      "IV1_ESP object\n",
      "IV3_ESP object\n",
      "IV7_ESP object\n",
      "II7_ESP object\n",
      "II8_ESP object\n",
      "PP09A_ESP object\n",
      "PP09C_ESP object\n",
      "IV1_ESP object\n",
      "IV3_ESP object\n",
      "IV7_ESP object\n",
      "II7_ESP object\n",
      "II8_ESP object\n",
      "PP09A_ESP object\n",
      "PP09C_ESP object\n",
      "IV1_ESP object\n",
      "IV3_ESP object\n",
      "IV7_ESP object\n",
      "II7_ESP object\n",
      "II8_ESP object\n",
      "PP09A_ESP object\n",
      "PP09C_ESP object\n",
      "IV1_ESP object\n",
      "IV3_ESP object\n",
      "IV7_ESP object\n",
      "II7_ESP object\n",
      "II8_ESP object\n",
      "PP09A_ESP object\n",
      "PP09C_ESP object\n",
      "IV1_ESP object\n",
      "IV3_ESP object\n",
      "IV7_ESP object\n",
      "II7_ESP object\n",
      "II8_ESP object\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos observaciones con edades (CH06) menores a 0:\n",
    "eph= eph.drop(eph[(eph[\"CH06\"] <0)].index)\n",
    "\n",
    "# Las variables de ingreso IPCF e ITF no toman valores negativos.\n",
    "\n",
    "# Reemplazamos los 9, 99, 999, 9999, 99999 por \"nan\" en aquellas variables en las que dichos codigos corresponden a valores faltantes.\n",
    "\n",
    "missing_codes= [9, 99, 999, 9999, 99999]\n",
    "\n",
    "for i in missing_codes:\n",
    "    for column in eph.columns:\n",
    "        if column!=\"CH06\":\n",
    "            try:\n",
    "                if np.nanmax(eph[column])==i:\n",
    "                    eph[column]=eph[column].replace(i, np.nan)\n",
    "            except:\n",
    "                print(column, eph[column].dtype) \n",
    "\n",
    "                \n",
    "eph[\"CH08\"]=eph[\"CH08\"].replace(9, np.nan) #La variable \"CH08\" toma valores mayores a 9, pero el valor 9 corresponde a los valores faltantes  \n",
    "eph[\"ESTADO\"]=eph[\"ESTADO\"].replace(0, np.nan) #En el caso de la variable \"ESTADO\", los valores faltantes se identifican con el código 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos las columnas que tienen solo missings values\n",
    "eph = eph.dropna(axis=1, how='all') \n",
    "\n",
    "# Borramos las variables que no son numéricas, salvo CODUSU\n",
    "eph_num = eph.select_dtypes(include=[\"number\"])\n",
    "eph = pd.concat([eph_num, eph[\"CODUSU\"]], axis=1)\n",
    "\n",
    "# ... Y otras que son numéricas, pero no aportan mucho en este caso (y no se utilizan en la identificacion de los hogares)\n",
    "eph = eph.drop(columns=[\"ANO4\", \"TRIMESTRE\", \"REGION\", \"PONDERA\", \"PONDIIO\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eph[[\"CODUSU\", \"NRO_HOGAR\", \"COMPONENTE\", \"II2\"]].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hay variables categoricas que tienen muchas categorías y que refieren a códigos de ocupaciones o países de origen. Las eliminamos porque convertirlas en dummies agregaría demasiadas variables al modelo:\n",
    "\n",
    "variables_eliminar=['PP11D_COD', 'PP11B_COD', 'PP04D_COD', 'PP04B_COD', 'CH16_COD', 'CH15_COD']\n",
    "\n",
    "#También eliminamos las variables de ingreso que luego tendremos que eliminar (a excepción de ITF, que la necesitamos), ya que no queremos luego perder observaciones porque tengan missing values en estas variables que de todas formas no vamos a usar:\n",
    "\n",
    "ingreso = [\"PP08D1\", \"PP08D4\", \"PP08F1\", \"PP08F2\", \"PP08J1\", \"PP08J2\", \"PP08J3\", #Ocupacion principal de asalariados\n",
    "            \"P21\", \"DECOCUR\", \"IDECOCUR\", \"RDECOCUR\", \"GDECOCUR\", \"PDECOCUR\", \"ADECOCUR\", \"PONDIIO\", # Ocupación principal\n",
    "            \"TOT_P12\", #Otra ocupaciones\n",
    "            \"P47T\", \"DECINDR\", \"IDECINDR\", \"RDECINDR\", \"GDECINDR\", \"PDECINDR\", \"ADECINDR\", \"PONDII\", #Individual\n",
    "            \"V2_M\", \"V3_M\", \"V4_M\", \"V5_M\", \"V8_M\", \"V9_M\", \"V10_M\", \"V11_M\", \"V12_M\", \"V18_M\", \"V19_AM\", \"V21_M\", \"T_VI\", #No laborales\n",
    "            \"DECIFR\", \"IDECIFR\", \"RDECIFR\", \"GDECIFR\", \"PDECIFR\", \"ADECIFR\", # Total Familiar\n",
    "            \"IPCF\", \"DECCFR\", \"IDECCFR\", \"RDECCFR\", \"GDECCFR\", \"PDECCFR\", \"ADECCFR\", #Per cap familiar\n",
    "            'PP06C', 'PP06D', #Trabajadores independientes\n",
    "            ]\n",
    "\n",
    "#Y tambien eliminamos las variables para las que más del 30% de las observaciones toman missing values:\n",
    "\n",
    "muchos_missings=[]\n",
    "for v in eph:\n",
    "    if eph[v].isnull().sum()>0.1*len(eph):\n",
    "        muchos_missings.append(v)\n",
    "        \n",
    "dropcolumns= variables_eliminar + ingreso + muchos_missings   \n",
    "dropcolumns=[x for x in dropcolumns if x in eph.columns]\n",
    "\n",
    "        \n",
    "eph=eph.drop(columns=dropcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NRO_HOGAR',\n",
       " 'COMPONENTE',\n",
       " 'H15',\n",
       " 'AGLOMERADO',\n",
       " 'CH03',\n",
       " 'CH04',\n",
       " 'CH06',\n",
       " 'CH07',\n",
       " 'CH08',\n",
       " 'CH09',\n",
       " 'CH10',\n",
       " 'CH11',\n",
       " 'CH12',\n",
       " 'CH13',\n",
       " 'CH15',\n",
       " 'CH16',\n",
       " 'NIVEL_ED',\n",
       " 'ESTADO',\n",
       " 'CAT_OCUP',\n",
       " 'CAT_INAC',\n",
       " 'PP02C1',\n",
       " 'PP02C2',\n",
       " 'PP02C3',\n",
       " 'PP02C4',\n",
       " 'PP02C5',\n",
       " 'PP02C6',\n",
       " 'PP02C7',\n",
       " 'PP02C8',\n",
       " 'PP02E',\n",
       " 'PP02H',\n",
       " 'PP02I',\n",
       " 'ITF',\n",
       " 'PONDIH',\n",
       " 'CODUSU']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eph.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La variable 'Nivel_ed' es una variable discreta ordinal, con la excepción de que el 7 corresponde a las personas sin instrucción. \n",
    "# Para solucionar eso, reemplazamos a 7 por 0, y así los valores están en orden desde el nivel más bajo al más alto de educación. \n",
    "\n",
    "eph['NIVEL_ED']=eph['NIVEL_ED'].replace(7, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de observaciones antes: 48259\n",
      "Numero de observaciones despues: 48117\n"
     ]
    }
   ],
   "source": [
    "#Una vez que eliminamos las columnas con muchos missing values, borramos las observaciones que tienen missing values en las otras variables\n",
    "print(\"Numero de observaciones antes:\", len(eph))\n",
    "eph = eph.dropna()\n",
    "print(\"Numero de observaciones despues:\", len(eph)) #No perdimos un número tan grande de observaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NRO_HOGAR',\n",
       " 'COMPONENTE',\n",
       " 'H15',\n",
       " 'AGLOMERADO',\n",
       " 'CH03',\n",
       " 'CH04',\n",
       " 'CH06',\n",
       " 'CH07',\n",
       " 'CH08',\n",
       " 'CH09',\n",
       " 'CH10',\n",
       " 'CH11',\n",
       " 'CH12',\n",
       " 'CH13',\n",
       " 'CH15',\n",
       " 'CH16',\n",
       " 'NIVEL_ED',\n",
       " 'ESTADO',\n",
       " 'CAT_OCUP',\n",
       " 'CAT_INAC',\n",
       " 'PP02C1',\n",
       " 'PP02C2',\n",
       " 'PP02C3',\n",
       " 'PP02C4',\n",
       " 'PP02C5',\n",
       " 'PP02C6',\n",
       " 'PP02C7',\n",
       " 'PP02C8',\n",
       " 'PP02E',\n",
       " 'PP02H',\n",
       " 'PP02I',\n",
       " 'ITF',\n",
       " 'PONDIH',\n",
       " 'CODUSU']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eph.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos a dummies las variables categóricas nominales \n",
    "\n",
    "variables= eph.columns.tolist()\n",
    "\n",
    "#Excluimos las variables continuas y las nominales ordinales\n",
    "continuous_variables=['CODUSU', 'NRO_HOGAR', 'COMPONENTE', 'ITF', 'NIVEL_ED', 'CH12', 'CH06', 'PP04B3_MES', 'PP04B3_ANO', 'PP04B3_DIA', 'PP05B2_MES', 'PP05B2_ANO', 'PP05B2_DIA', 'PP03D', 'PP3E_TOT', 'PP3F_TOT', 'PP11B2_MES', 'PP11B2_ANO', 'PP11B2_DIA',  'PP11G_ANO', 'PP11G_MES', 'PP11G_DIA', 'PP04C', 'IV2', 'II1', 'II3_1', 'PONDIH']\n",
    "\n",
    "#Guardamos las variables categóricas nominales\n",
    "categorical_variables=[x for x in variables if x not in continuous_variables]\n",
    "\n",
    "eph_cat=eph[categorical_variables].astype(str)\n",
    "eph_cat_dummies=pd.get_dummies(eph_cat, drop_first=True) #Hacemos variables dummies para cada valor que toma cada variable categórica nominal\n",
    "\n",
    "#Guardamos a las dummies de las variables discretas nominales y a las variables continuas y discretas ordinales\n",
    "continuous_variables= [x for x in continuous_variables if x in variables]\n",
    "\n",
    "eph_old=eph\n",
    "\n",
    "eph = pd.concat([eph_cat_dummies, eph[continuous_variables]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H15_1',\n",
       " 'AGLOMERADO_12',\n",
       " 'AGLOMERADO_13',\n",
       " 'AGLOMERADO_14',\n",
       " 'AGLOMERADO_15',\n",
       " 'AGLOMERADO_17',\n",
       " 'AGLOMERADO_18',\n",
       " 'AGLOMERADO_19',\n",
       " 'AGLOMERADO_2',\n",
       " 'AGLOMERADO_20',\n",
       " 'AGLOMERADO_22',\n",
       " 'AGLOMERADO_23',\n",
       " 'AGLOMERADO_25',\n",
       " 'AGLOMERADO_26',\n",
       " 'AGLOMERADO_27',\n",
       " 'AGLOMERADO_29',\n",
       " 'AGLOMERADO_3',\n",
       " 'AGLOMERADO_30',\n",
       " 'AGLOMERADO_31',\n",
       " 'AGLOMERADO_32',\n",
       " 'AGLOMERADO_33',\n",
       " 'AGLOMERADO_34',\n",
       " 'AGLOMERADO_36',\n",
       " 'AGLOMERADO_38',\n",
       " 'AGLOMERADO_4',\n",
       " 'AGLOMERADO_5',\n",
       " 'AGLOMERADO_6',\n",
       " 'AGLOMERADO_7',\n",
       " 'AGLOMERADO_8',\n",
       " 'AGLOMERADO_9',\n",
       " 'AGLOMERADO_91',\n",
       " 'AGLOMERADO_93',\n",
       " 'CH03_10',\n",
       " 'CH03_2',\n",
       " 'CH03_3',\n",
       " 'CH03_4',\n",
       " 'CH03_5',\n",
       " 'CH03_6',\n",
       " 'CH03_7',\n",
       " 'CH03_8',\n",
       " 'CH03_9',\n",
       " 'CH04_2',\n",
       " 'CH07_2.0',\n",
       " 'CH07_3.0',\n",
       " 'CH07_4.0',\n",
       " 'CH07_5.0',\n",
       " 'CH08_12.0',\n",
       " 'CH08_13.0',\n",
       " 'CH08_2.0',\n",
       " 'CH08_23.0',\n",
       " 'CH08_3.0',\n",
       " 'CH08_4.0',\n",
       " 'CH09_2.0',\n",
       " 'CH09_3.0',\n",
       " 'CH10_1.0',\n",
       " 'CH10_2.0',\n",
       " 'CH10_3.0',\n",
       " 'CH11_1.0',\n",
       " 'CH11_2.0',\n",
       " 'CH13_1.0',\n",
       " 'CH13_2.0',\n",
       " 'CH15_2.0',\n",
       " 'CH15_3.0',\n",
       " 'CH15_4.0',\n",
       " 'CH15_5.0',\n",
       " 'CH16_1.0',\n",
       " 'CH16_2.0',\n",
       " 'CH16_3.0',\n",
       " 'CH16_4.0',\n",
       " 'CH16_5.0',\n",
       " 'CH16_6.0',\n",
       " 'ESTADO_2.0',\n",
       " 'ESTADO_3.0',\n",
       " 'ESTADO_4.0',\n",
       " 'CAT_OCUP_1.0',\n",
       " 'CAT_OCUP_2.0',\n",
       " 'CAT_OCUP_3.0',\n",
       " 'CAT_OCUP_4.0',\n",
       " 'CAT_INAC_1',\n",
       " 'CAT_INAC_2',\n",
       " 'CAT_INAC_3',\n",
       " 'CAT_INAC_4',\n",
       " 'CAT_INAC_5',\n",
       " 'CAT_INAC_6',\n",
       " 'CAT_INAC_7',\n",
       " 'PP02C1_1',\n",
       " 'PP02C1_2',\n",
       " 'PP02C2_1',\n",
       " 'PP02C2_2',\n",
       " 'PP02C3_1',\n",
       " 'PP02C3_2',\n",
       " 'PP02C4_1',\n",
       " 'PP02C4_2',\n",
       " 'PP02C5_1',\n",
       " 'PP02C5_2',\n",
       " 'PP02C6_1',\n",
       " 'PP02C6_2',\n",
       " 'PP02C7_1',\n",
       " 'PP02C7_2',\n",
       " 'PP02C8_1',\n",
       " 'PP02C8_2',\n",
       " 'PP02E_1',\n",
       " 'PP02E_2',\n",
       " 'PP02E_3',\n",
       " 'PP02E_4',\n",
       " 'PP02E_5',\n",
       " 'PP02H_1',\n",
       " 'PP02H_2',\n",
       " 'PP02I_1',\n",
       " 'PP02I_2',\n",
       " 'CODUSU',\n",
       " 'NRO_HOGAR',\n",
       " 'COMPONENTE',\n",
       " 'ITF',\n",
       " 'NIVEL_ED',\n",
       " 'CH12',\n",
       " 'CH06',\n",
       " 'PONDIH']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eph.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ahora vamos a construir algunas variables que son relevantes para predecir a individuos que están bajo la línea de pobreza. Las variables que creamos son las siguientes:\n",
    "\n",
    "* `propunder10`: Esta variable indica la proporción de personas con menos de 10 años por hogar. *Nota*: Originalmente, en la EPH existía esta variable, pero, al parecer, no fue creada correctamente porque tenía muchos missings. Nosotros la creamos manualmente usando la variable de edad (`CH06`)\n",
    "* `overcrowded`: Se consideran hogares con hacinamiento a aquellos que tienen 2 personas o más por cuarto, y con hacinamiento crítico a los que tienen más de 3 personas por cuarto. (Secretaría Nacional de Niñez, Adolescencia y Familia). Esta variable tomará valor 0 si no está hacinado, 1 si tiene hacinamiento y 2 si tiene hacinamiento crítico.\n",
    "* `var3`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODUSU</th>\n",
       "      <th>NRO_HOGAR</th>\n",
       "      <th>COMPONENTE</th>\n",
       "      <th>CH06</th>\n",
       "      <th>propunder10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13555</th>\n",
       "      <td>TQRMNOPPQHJKLLCDEFKID00798130</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13556</th>\n",
       "      <td>TQRMNOPPQHJKLLCDEFKID00798130</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7216</th>\n",
       "      <td>TQRMNOPPQHJLKUCDEFKID00796257</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>TQRMNOPPQHJLKUCDEFKID00796257</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25117</th>\n",
       "      <td>TQRMNOPPQHJMLOCDEHPJB00795718</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25118</th>\n",
       "      <td>TQRMNOPPQHJMLOCDEHPJB00795718</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25119</th>\n",
       "      <td>TQRMNOPPQHJMLOCDEHPJB00795718</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25389</th>\n",
       "      <td>TQRMNOPPQHJMLOCDEHPJB00795718</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25390</th>\n",
       "      <td>TQRMNOPPQHJMLOCDEHPJB00795718</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22422</th>\n",
       "      <td>TQRMNOPPQHJNLPCDEHJGH00793308</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22423</th>\n",
       "      <td>TQRMNOPPQHJNLPCDEHJGH00793308</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22424</th>\n",
       "      <td>TQRMNOPPQHJNLPCDEHJGH00793308</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22425</th>\n",
       "      <td>TQRMNOPPQHJNLPCDEHJGH00793308</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22426</th>\n",
       "      <td>TQRMNOPPQHJNLPCDEHJGH00793308</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22427</th>\n",
       "      <td>TQRMNOPPQHJNLPCDEHJGH00793308</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              CODUSU  NRO_HOGAR  COMPONENTE  CH06  propunder10\n",
       "13555  TQRMNOPPQHJKLLCDEFKID00798130          1           1    73          0.0\n",
       "13556  TQRMNOPPQHJKLLCDEFKID00798130          1           2    64          0.0\n",
       "7216   TQRMNOPPQHJLKUCDEFKID00796257          1           1    51          0.0\n",
       "7217   TQRMNOPPQHJLKUCDEFKID00796257          1           2    48          0.0\n",
       "25117  TQRMNOPPQHJMLOCDEHPJB00795718          1           1    30          0.2\n",
       "25118  TQRMNOPPQHJMLOCDEHPJB00795718          1           2    64          0.2\n",
       "25119  TQRMNOPPQHJMLOCDEHPJB00795718          1           3    66          0.2\n",
       "25389  TQRMNOPPQHJMLOCDEHPJB00795718          1           4    29          0.2\n",
       "25390  TQRMNOPPQHJMLOCDEHPJB00795718          1           5     9          0.2\n",
       "22422  TQRMNOPPQHJNLPCDEHJGH00793308          1           1    37          0.5\n",
       "22423  TQRMNOPPQHJNLPCDEHJGH00793308          1           2    35          0.5\n",
       "22424  TQRMNOPPQHJNLPCDEHJGH00793308          1           3    11          0.5\n",
       "22425  TQRMNOPPQHJNLPCDEHJGH00793308          1           4     9          0.5\n",
       "22426  TQRMNOPPQHJNLPCDEHJGH00793308          1           5     7          0.5\n",
       "22427  TQRMNOPPQHJNLPCDEHJGH00793308          1           6     3          0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos la variable propunder10 que indica la proporción de personas menores a 10 años en cada hogar\n",
    "# Sort the observations by CODUSU NRO_HOGAR and COMPONENTE\n",
    "eph = eph.sort_values(by=[\"CODUSU\", \"NRO_HOGAR\", \"COMPONENTE\"])\n",
    "\n",
    "# Create a variable that indicates the proportion of people behind 10 years old (CH06 < 10) by household (CODUSU, NRO_HOGAR)\n",
    "eph[\"propunder10\"] = eph.groupby([\"CODUSU\", \"NRO_HOGAR\"])[\"CH06\"].transform(lambda x: (x < 10).sum() / len(x))\n",
    "eph[[\"CODUSU\", \"NRO_HOGAR\", \"COMPONENTE\", \"CH06\", \"propunder10\"]].head(15) #Confirmamos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H15_1',\n",
       " 'AGLOMERADO_12',\n",
       " 'AGLOMERADO_13',\n",
       " 'AGLOMERADO_14',\n",
       " 'AGLOMERADO_15',\n",
       " 'AGLOMERADO_17',\n",
       " 'AGLOMERADO_18',\n",
       " 'AGLOMERADO_19',\n",
       " 'AGLOMERADO_2',\n",
       " 'AGLOMERADO_20',\n",
       " 'AGLOMERADO_22',\n",
       " 'AGLOMERADO_23',\n",
       " 'AGLOMERADO_25',\n",
       " 'AGLOMERADO_26',\n",
       " 'AGLOMERADO_27',\n",
       " 'AGLOMERADO_29',\n",
       " 'AGLOMERADO_3',\n",
       " 'AGLOMERADO_30',\n",
       " 'AGLOMERADO_31',\n",
       " 'AGLOMERADO_32',\n",
       " 'AGLOMERADO_33',\n",
       " 'AGLOMERADO_34',\n",
       " 'AGLOMERADO_36',\n",
       " 'AGLOMERADO_38',\n",
       " 'AGLOMERADO_4',\n",
       " 'AGLOMERADO_5',\n",
       " 'AGLOMERADO_6',\n",
       " 'AGLOMERADO_7',\n",
       " 'AGLOMERADO_8',\n",
       " 'AGLOMERADO_9',\n",
       " 'AGLOMERADO_91',\n",
       " 'AGLOMERADO_93',\n",
       " 'CH03_10',\n",
       " 'CH03_2',\n",
       " 'CH03_3',\n",
       " 'CH03_4',\n",
       " 'CH03_5',\n",
       " 'CH03_6',\n",
       " 'CH03_7',\n",
       " 'CH03_8',\n",
       " 'CH03_9',\n",
       " 'CH04_2',\n",
       " 'CH07_2.0',\n",
       " 'CH07_3.0',\n",
       " 'CH07_4.0',\n",
       " 'CH07_5.0',\n",
       " 'CH08_12.0',\n",
       " 'CH08_13.0',\n",
       " 'CH08_2.0',\n",
       " 'CH08_23.0',\n",
       " 'CH08_3.0',\n",
       " 'CH08_4.0',\n",
       " 'CH09_2.0',\n",
       " 'CH09_3.0',\n",
       " 'CH10_1.0',\n",
       " 'CH10_2.0',\n",
       " 'CH10_3.0',\n",
       " 'CH11_1.0',\n",
       " 'CH11_2.0',\n",
       " 'CH13_1.0',\n",
       " 'CH13_2.0',\n",
       " 'CH15_2.0',\n",
       " 'CH15_3.0',\n",
       " 'CH15_4.0',\n",
       " 'CH15_5.0',\n",
       " 'CH16_1.0',\n",
       " 'CH16_2.0',\n",
       " 'CH16_3.0',\n",
       " 'CH16_4.0',\n",
       " 'CH16_5.0',\n",
       " 'CH16_6.0',\n",
       " 'ESTADO_2.0',\n",
       " 'ESTADO_3.0',\n",
       " 'ESTADO_4.0',\n",
       " 'CAT_OCUP_1.0',\n",
       " 'CAT_OCUP_2.0',\n",
       " 'CAT_OCUP_3.0',\n",
       " 'CAT_OCUP_4.0',\n",
       " 'CAT_INAC_1',\n",
       " 'CAT_INAC_2',\n",
       " 'CAT_INAC_3',\n",
       " 'CAT_INAC_4',\n",
       " 'CAT_INAC_5',\n",
       " 'CAT_INAC_6',\n",
       " 'CAT_INAC_7',\n",
       " 'PP02C1_1',\n",
       " 'PP02C1_2',\n",
       " 'PP02C2_1',\n",
       " 'PP02C2_2',\n",
       " 'PP02C3_1',\n",
       " 'PP02C3_2',\n",
       " 'PP02C4_1',\n",
       " 'PP02C4_2',\n",
       " 'PP02C5_1',\n",
       " 'PP02C5_2',\n",
       " 'PP02C6_1',\n",
       " 'PP02C6_2',\n",
       " 'PP02C7_1',\n",
       " 'PP02C7_2',\n",
       " 'PP02C8_1',\n",
       " 'PP02C8_2',\n",
       " 'PP02E_1',\n",
       " 'PP02E_2',\n",
       " 'PP02E_3',\n",
       " 'PP02E_4',\n",
       " 'PP02E_5',\n",
       " 'PP02H_1',\n",
       " 'PP02H_2',\n",
       " 'PP02I_1',\n",
       " 'PP02I_2',\n",
       " 'CODUSU',\n",
       " 'NRO_HOGAR',\n",
       " 'COMPONENTE',\n",
       " 'ITF',\n",
       " 'NIVEL_ED',\n",
       " 'CH12',\n",
       " 'CH06',\n",
       " 'PONDIH',\n",
       " 'prop_under_10',\n",
       " 'propunder10']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eph.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Estadísticas descriptivas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Construimos la variable `adulto_equiv` y la columna `ad_equiv_hogar` y creamos los datasets para las personas que respondieron y no respondieron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6) Repetimos el código del inciso 1.2.f del TP2\n",
    "\n",
    "#Abrimos el archivo \"tabla adulto equiv.xlsx\"\n",
    "\n",
    "adulto = pd.read_excel(\"../datasets/tabla_adulto_equiv.xlsx\")\n",
    "\n",
    "#Acomodamos la base:\n",
    "\n",
    "adulto=adulto[4:27] #Nos quedamos solo con las celdas que nos interesan\n",
    "adulto[\"Edad\"]=adulto[\"Tabla de equivalencias de necesidades energéticas. Unidades de adulto equivalente, según sexo y edad\"]\n",
    "adulto[\"1\"]=adulto[\"Unnamed: 1\"]\n",
    "adulto[\"0\"]=adulto[\"Unnamed: 2\"]\n",
    "adulto= adulto[[\"Edad\", \"1\", \"0\"]]\n",
    "\n",
    "adulto[\"Edad\"]=adulto[\"Edad\"].str.replace(\"años\", \"\").str.replace(\"año\", \"\").str.replace(\" \", \"\").str.replace(\"a\", \"\")\n",
    "adulto = adulto.set_index(\"Edad\")\n",
    "\n",
    "def equivalencia(edad, genero):\n",
    "    if edad <18: #No hay observaciones con edades menor a 1, asi que la aquivalencia para las personas de menos de 1 año de edad no es un problema en este caso\n",
    "        equiv= adulto.at[str(edad),str(genero)]\n",
    "    if edad>=18 and edad <30:\n",
    "        equiv= adulto.at[\"1829\", str(genero)]\n",
    "    if edad>=30 and edad <46:\n",
    "        equiv= adulto.at[\"3045\", str(genero)]   \n",
    "    if edad>=46 and edad <61:\n",
    "        equiv= adulto.at[\"4660\", str(genero)]   \n",
    "    if edad>=61 and edad <76: #No queda claro en la tabla, pero asumimos que las personas de 75 están incluidas en esta categoría\n",
    "        equiv= adulto.at[\"6175\", str(genero)]    \n",
    "    if edad>=76:\n",
    "        equiv= adulto.at[\"másde75\", str(genero)] \n",
    "    return(equiv)  \n",
    "\n",
    "eph[\"adulto_equiv\"]= eph.apply(lambda x: equivalencia(x.CH06, x.CH04_2), axis=1)\n",
    "\n",
    "#Generamos una base que suma las equivalencias por hogar, y luego unimos esa base a la nuestra.\n",
    "\n",
    "suma=eph.groupby(by=[\"CODUSU\", \"NRO_HOGAR\"]).agg({\"adulto_equiv\":\"sum\"})\n",
    "suma[\"ad_equiv_hogar\"]=suma[\"adulto_equiv\"]\n",
    "suma = suma.drop('adulto_equiv', axis=1)\n",
    "\n",
    "# Y hacemos un merge con el df \"suma\" que contiene la variable de factor de equivalencia por hogar (ad_equiv_hogar)\n",
    "eph=eph.merge(suma, on=['CODUSU','NRO_HOGAR'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8) Repetimos los incisos 1.3 y 1.4 del TP2\n",
    "\n",
    "#Guarden como una base distinta llamada respondieron las observaciones donde respondieron la pregunta sobre su ITF. \n",
    "#Las observaciones con ITF = 0 guardenlas en una base bajo el nombre norespondieron.\n",
    "\n",
    "respondieron=eph[eph[\"ITF\"]>0]\n",
    "norespondieron=eph[eph[\"ITF\"]==0]\n",
    "\n",
    "respondieron[\"ingreso_necesario\"]= 57371.05*respondieron[\"ad_equiv_hogar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#9) Agreguen a la base respondieron una columna llamada pobre, que tome valor 1 si el ITF es menor al ingreso necesario que necesita esa familia y 0 en caso contrario\n",
    "\n",
    "respondieron[\"pobre\"] = (respondieron[\"ITF\"]< respondieron[\"ingreso_necesario\"]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10) Nos quedamos con una observación por hogar para calcular la tasa de pobreza:\n",
    "\n",
    "hogares = respondieron.groupby(['CODUSU', 'NRO_HOGAR']).head(1)\n",
    "pobreza= ((hogares[\"pobre\"]*hogares[\"PONDIH\"]).sum()/hogares[\"PONDIH\"].sum())*100\n",
    "print(\"Tasa de pobreza para el GBA:\", pobreza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte II: Construcción de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo para asegurarnos que las funciones están funcionando correctamente, las probaremos en unos vectores creados aleatoriamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos los objetos X e y\n",
    "np.random.seed(42)\n",
    "y = np.round(np.random.rand(1000)) #dummy\n",
    "X = np.random.rand(1000, 5)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "\n",
    "# Dividimos la base en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Creamos la función `evalua_metodo()`, la cual nos permitirá crear métricas necesarias para evaluar distintos modelos de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_metodo(model, X_train, y_train, X_test, y_test, showmc=False, showroc=False):\n",
    "    '''\n",
    "    El objetivo de esta función es analizar un modelo de clasificación binaria y generar métricas para evaluar su desempeño.\n",
    "    Argumentos:\n",
    "    - modelo definido\n",
    "    - Muestras de entrenamiento (X_train y y_train)\n",
    "    - Muestras de testeo (X_test y y_test)\n",
    "    \n",
    "    La función devuelve como outputs las siguientes métricas:\n",
    "    - matriz de confusión\n",
    "    - accuracy\n",
    "    - área bajo la curva ROC (AUC)\n",
    "    - ECM\n",
    "    '''\n",
    "    \n",
    "    modelfit = model.fit(X_train, y_train) #previamente definimos el modelo (logit, KNN, LDA)\n",
    "    y_pred = modelfit.predict(X_test)\n",
    "\n",
    "    # Métricas para evaluación:\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "    ecm = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # Visualización de matriz de confusión:\n",
    "    if showmc==True:\n",
    "        mc_plot = sns.heatmap(matriz_confusion, annot=True, fmt='g', cmap='Blues', xticklabels=['Falso', 'Verdadero'], yticklabels=['Falso', 'Verdadero'], annot_kws={\"fontsize\": 12})\n",
    "        mc_plot.set_xlabel('Predicción')\n",
    "        mc_plot.set_ylabel('Observado')\n",
    "        plt.show()\n",
    "\n",
    "    # Visualización de curva ROC:\n",
    "    if showroc==True:\n",
    "        fpr_mod, tpr_mod, thresholds_mod = roc_curve(y_test, y_pred)\n",
    "        display = RocCurveDisplay(fpr=fpr_mod, tpr=tpr_mod, roc_auc=auc)\n",
    "        display.plot()\n",
    "        plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "        plt.show() \n",
    "    \n",
    "    # Visualización de métricas:\n",
    "    model_metrics = pd.DataFrame({'model': [model], 'accuracy': [accuracy], 'auc':[auc], 'ecm':[ecm]})\n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos que el comando `evalua_metodo()` funcione correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalua_metodo(LogisticRegression(), X_train, y_train, X_test, y_test) #puede ser cualquier modelo: KNN o LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le agregué una opción para visualizar la matriz de confusión, por si sirve.\n",
    "evalua_metodo(LogisticRegression(), X_train, y_train, X_test, y_test, showmc=True, showroc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creamos la función `cross_validation()` la cual nos permitirá hacer la validación cruzada con *k* iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, k, X, y):\n",
    "    '''\n",
    "    El objetivo de esta función es realizar un k-fold cross validation para evaluar el desempeño de un modelo de clasificación binaria.\n",
    "    La función parte el dataset en K particiones de entrenamiento y test, y aplica la función evalua_metodo para cada una de las particiones.\n",
    "    \n",
    "    Argumentos:\n",
    "    - modelo: El modelo de clasificación a evaluar.\n",
    "    - k: parámetro para cross validation.\n",
    "    - X: conjunto de datos de características.\n",
    "    - y: La variable objetivo. \n",
    "    \n",
    "    La función retorna un df que contiene el número de iteraciones (k), precisión y ECM promedio.\n",
    "    \n",
    "    '''\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model_metrics = evalua_metodo(model, X_train, y_train, X_test, y_test)\n",
    "        model_metrics['k'] = i\n",
    "        results.append(model_metrics)\n",
    "    \n",
    "    combined_results = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    # Calculamos la ECM promedio\n",
    "    mean_accuracy = combined_results['accuracy'].mean()\n",
    "    mean_ecm = combined_results['ecm'].mean()\n",
    "    mean_auc=combined_results['auc'].mean()\n",
    "    \n",
    "    return pd.DataFrame({'model': [model], 'k': [k], 'accuracy_mean': [mean_accuracy], 'ecm_mean': [mean_ecm], 'auc_mean': [mean_auc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos que la función `cross_validation()` funcione correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(LogisticRegression(max_iter=10000), 5, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creamos la función `evalua_config()` que recibirá distintas combinaciones de hiperparámetros. Utilizamos la función `cross_validation()` previamente creada para cada configuración. La función devuelve la configuración que genera el menor ECM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_config(configs, X_train, y_train, k):\n",
    "    \"\"\"\n",
    "    El objetivo es evaluar distintas configuraciones de hiperparámetros \n",
    "    que definen a un modelo y devolver la mejor configuración con menor ECM.\n",
    "    \n",
    "    Argumentos:\n",
    "    - configs (list): las distintas configuraciones a evaluar.\n",
    "    - Muestras de entrenamiento (X_train y y_train)\n",
    "    - k: parámetro para cross validation.\n",
    "    \n",
    "    El output es un diccionario con la mejor configuración y el menor ECM.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_config = None\n",
    "    menor_ecm = np.inf\n",
    "    \n",
    "    for config in configs:\n",
    "        modelo = LogisticRegression(max_iter=1000, **config)\n",
    "        resultados = cross_validation(modelo, k, X_train, y_train)\n",
    "        ecm_promedio = resultados[\"ecm_mean\"].mean() # El ECM promedio de las k iteraciones\n",
    "\n",
    "    # Actualizamos la mejor configuración y el menor ECM:\n",
    "        if ecm_promedio < menor_ecm:\n",
    "            menor_ecm=ecm_promedio\n",
    "            best_config=config \n",
    "    \n",
    "    return {\n",
    "        'Mejor configuración': best_config,\n",
    "        'ECM menor': menor_ecm \n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos que la función `evalua_config` esté funcionando correctamente. Creamos algunas configuraciones para que nos dé cuál es la mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configlist = [\n",
    "     {'penalty': 'l1', 'C': 0.5, 'solver': 'saga'},\n",
    "     {'penalty': 'l1', 'C': 1, 'solver': 'saga'},  \n",
    "     {'penalty': 'l2', 'C': 0.5},\n",
    "     {'penalty': 'l2', 'C': 1}  \n",
    "    ]\n",
    "\n",
    "resultado = evalua_config(configlist, X_train, y_train,10)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Escriban una función llamada `evalua_multiples_métodos` que les permita implementar los siguiente métodos con los hiperparámetros que ustedes elijan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_multiples_metodos(configs_log, X_train, y_train, X_test, y_test, k, vecinos, componentes):\n",
    "      \n",
    "    \"\"\"\n",
    "    El objetivo es implementar distintos metodos con los hiperparametros a eleccion y  \n",
    "    obtener metricas que permitan evaluarlos.\n",
    "    \n",
    "    Argumentos:\n",
    "    - configs_log (list): las distintas configuraciones a evaluar para la regresion logistica.\n",
    "    - Muestras de entrenamiento (X_train y y_train)\n",
    "    - Muestras de testeo (X_test y y_test)\n",
    "    - k: parámetro para cross validation.\n",
    "    - vecinos (list): distintos numeros de vecinos a evaluar para el metodo de Vecinos Cercanos\n",
    "    - componentes: numero de componentes para el Analisis Discriminante Lineal\n",
    "    \n",
    "    El output es un data frame que contiene el modelo, el valor de los hiperparametros, el ECM, el accuracy, y el AUC para cada modelo.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    result= pd.DataFrame(columns=[\"modelo\", \"penalty\", \"C\", \"vecinos\", \"componentes\", \"ecm\", \"accuracy\", \"auc\"])\n",
    "    \n",
    "    #Regresion logistica:\n",
    "    mejor_conf= evalua_config(configs_log, X_train, y_train, k)\n",
    "    mejor_conf= mejor_conf[\"Mejor configuración\"]\n",
    "    best_logistic_model=LogisticRegression(max_iter=10000, **mejor_conf) \n",
    "    metrics= evalua_metodo(best_logistic_model, X_train, y_train, X_test, y_test)\n",
    "    result = result.append({\"modelo\": \"Logit\", \"penalty\": mejor_conf[\"penalty\"], \"C\": mejor_conf[\"C\"], \"ecm\": metrics.loc[metrics.index[0], \"ecm\"], \"accuracy\": metrics.loc[metrics.index[0], \"accuracy\"], \"auc\": metrics.loc[metrics.index[0], \"auc\"]}, ignore_index=True) \n",
    "    \n",
    "    #KNN:\n",
    "    for v in vecinos:\n",
    "        knn = KNeighborsClassifier(n_neighbors=v)\n",
    "        metrics=evalua_metodo(knn, X_train, y_train, X_test, y_test)\n",
    "        result = result.append({\"modelo\": \"KNN\", \"vecinos\": v, \"ecm\": metrics.loc[metrics.index[0], \"ecm\"], \"accuracy\": metrics.loc[metrics.index[0], \"accuracy\"], \"auc\": metrics.loc[metrics.index[0], \"auc\"]}, ignore_index=True) \n",
    "\n",
    "   #LDA:\n",
    "    lda=LinearDiscriminantAnalysis(n_components=componentes)\n",
    "    metrics=evalua_metodo(lda, X_train, y_train, X_test, y_test)\n",
    "    result = result.append({\"modelo\": \"LDA\", \"componentes\": componentes, \"ecm\": metrics.loc[metrics.index[0], \"ecm\"], \"accuracy\": metrics.loc[metrics.index[0], \"accuracy\"], \"auc\": metrics.loc[metrics.index[0], \"auc\"]}, ignore_index=True) \n",
    "\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probamos que evalua_multiples_metodos funcione:\n",
    "\n",
    "configlist = [\n",
    "    {'penalty': 'l1', 'C': 0.5, 'solver': 'saga'},\n",
    "    {'penalty': 'l1', 'C': 1, 'solver': 'saga'},\n",
    "    {'penalty': 'l2', 'C': 0.5},\n",
    "    {'penalty': 'l2', 'C': 1} \n",
    "    ]\n",
    "\n",
    "vecinos_prueba= [1, 5, 10]\n",
    "\n",
    "evalua_multiples_metodos(configlist, X_train, y_train, X_test, y_test, 5, vecinos_prueba, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte III: Clasificación y regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta parte del trabajo es nuevamente intentar predecir si una persona es o no pobre utilizando datos distintos al ingreso, dado que muchos hogares son reacios a responder cuánto ganan. Esta vez lo haremos con la base unida de las preguntas de la encuesta individual y la encuesta de hogar.  su vez, incluiremos ejercicios de regularización y de validación cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Eliminamos de las bases `respondieron` y `norespondieron` las variables relacionadas a ingresos y las variables creadas en base al método de adulto equivalente. Creamos la variable dependiente (pobre) y la matriz de variables independientes. En la parte I, como parte de la limpieza de datos, ya eliminamos casi todas estas variables, salvo ITF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos ITF de ambas bases, y tambien PONDIH, que no es util para la prediccion.\n",
    "#Tambien eliminamos las variables \"adulto_equiv\", \"ad_equiv_hogar\" y \"ingreso_necesario\"\n",
    "respondieron=respondieron.drop(columns=[\"ITF\", \"PONDIH\", \"adulto_equiv\", \"ad_equiv_hogar\", \"ingreso_necesario\"])\n",
    "norespondieron=norespondieron.drop(columns=[\"ITF\", \"adulto_equiv\", \"ad_equiv_hogar\", \"PONDIH\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable dependiente: pobre\n",
    "y = respondieron[\"pobre\"]\n",
    "y=y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de variables independientes:\n",
    "X = respondieron.drop(columns=[\"pobre\", \"CODUSU\", \"NRO_HOGAR\"])\n",
    "X= sm.add_constant(X) \n",
    "\n",
    "#Dividimos la muestra en test y control\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estandarizamos las variables:\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Estandarizamos las observaciones de entrenamiento\n",
    "X_train_tran = pd.DataFrame(sc.fit_transform(X_train),index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "# Estandarizamos las observaciones de test\n",
    "X_test_tran= pd.DataFrame(sc.transform(X_test),index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "# Estadisticas luego de estandarizar\n",
    "X_train_tran.describe().round(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Corremos la función `evalua_multiples_metodos()` con la base respondieron. Este ejercicio nos permite verificar que la función no tienen ningún problema; sin embargo, los valores que toman los hiperparámetros son arbitrarias. Luego veremos cómo elegir la configuración óptima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "configlist = [\n",
    "    {'penalty': 'l2', 'C': 0.1},\n",
    "    {'penalty': 'l2', 'C': 100}, \n",
    "    {'penalty': 'l1', 'C': 0.1, 'solver': 'saga'},\n",
    "    {'penalty': 'l1', 'C': 100, 'solver': 'saga' }\n",
    "    ]\n",
    "\n",
    "vecinos_prueba= [3, 5]\n",
    "\n",
    "evalua_multiples_metodos(configlist, X_train_tran, y_train, X_test_tran, y_test, 5, vecinos_prueba, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Lambda es el valor de penalización a la \"complejidad\" del modelo. O sea, es lo nuestro modelo \"paga\" por añadir una variable adicional al modelo. Una forma de elegir el valor de lambda óptimo es usar cross-validation.\n",
    "\n",
    "Primero, deberíamos dividir los datos en un conjunto de training y test. Luego, podríamos generar una lista con posibles valores de lambda, por ejemplo, los valores [0.001, 0.01, 0.1, 1, 10, 100].\n",
    "\n",
    "Segundo, deberíamos decidir cuántos *folds (k)* vamos a usar para la cross-validation. Como mencionamos en el siguiente inciso, la regla práctica es usar un valor de 5 o 10.\n",
    "\n",
    "Tercero, para cada valor de lambda, haremos la cross-validation. Esto quiere decir que, para cada iteración, dividiremos la muestra de entrenamiento en k *folds*, luego entrenar el modelo con regularización (puede ser Ridge o Lasso) en los *k*-1 *folds*, evaluar el rendimiento en el *fold* restante y obtener una métrica como el ECM o accuracy. \n",
    "\n",
    "Finalmente, promediar las métricas de todas las iteraciones para cada valor de lambda.\n",
    "\n",
    "De esta forma, podremos identificar al valor de lambda que nos produzca la mejor métrica. Por ejemplo, el lambda asociado al menor ECM sería el **lambda óptimo**, el cual eligiríamos como la mejor configuración.\n",
    "\n",
    "No se debe usar el conjunto de test para elegir el valor de lambda debido a que eso podría llevar a una situación de *overfitting* del modelo al conjunto de test. Esto quiere decir que es como si el modelo se ajustara al conjunto de test. Nuestro objetivo es generar un modelo que nos ayude a predecir cuando tengamos nuevos datos, por lo tanto, el conjunto de test debe ser reservado para este propósito. De esta manera, nuestras estimaciones cuando tengamos nueva data serán más confiables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** En *cross-validation* es importante la elección de los *folds (k)* para poder entrenar y posteriormente evaluar el rendimiento de un modelo. \n",
    "\n",
    "Si se elige un *k* muy pequeño, entonces tendremos una mayor cantidad de datos para la muestra de test, pero una menor cantidad para la muestra de entrenamiento, por lo que es posible tener el problema de que la precisión del modelo sea menor. En este caso, la posibilidad de que la distribución de los datos para test difiera del conjunto de training es mayor. Esto quiere decir que las predicciones serían sensibles a valores particulares, por ejemplo, algunos valores outliers.\n",
    "\n",
    "Por otro lado, un *k* muy grande implica que cada modelo usa una base de training mayor y la base de test es menor. Hacer esto puede tener un costo computacional enorme si trabajamos con muchas observaciones y muchos predictores. Finalmente, cuando *k* es muy grande, puede existir una alta correlación entre *folds* lo que nos llevaría a pensar que, en el margen, una nueva partición no agrega tanta información al modelo. El caso extremo es el *leave-one-out* donde *k* = N, es decir, se deja una sola observación para test y se estima el modelo N veces con N-1 datos. En este último caso, el costo computacional es extremadamente alto.\n",
    "\n",
    "[Kohavi (1995)](https://dl.acm.org/doi/10.5555/1643031.1643047) mostró que, para seleccionar un modelo, la validación cruzada de cinco o diez veces (*k*) puede ser mejor que la validación leave-one-out. En ese sentido, para evitar los costos computacionales de usar un *k* demasiado alto, está ampliamente difundido la regla práctica de usar un *k* de 5 o 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Crearemos un vector con distintos valores de $\\lambda = 10^{n}$ donde $n$ pertenece al intervalo $\\{-5, \\dots, 5\\}$. Además, usamos diez folds para cross-validation ($k=10$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configlist_LASSO = []\n",
    "\n",
    "for n in range (-5, 6):\n",
    "    conf={}\n",
    "    conf[\"penalty\"]=\"l1\"\n",
    "    conf[\"C\"]= 1/(10**n)\n",
    "    conf[\"solver\"]= \"saga\"\n",
    "    configlist_LASSO.append(conf)\n",
    "    \n",
    "configlist_RIDGE = []\n",
    "\n",
    "for n in range (-5, 6):\n",
    "    conf={}\n",
    "    conf[\"penalty\"]=\"l2\"\n",
    "    conf[\"C\"]= 1/(10**n)\n",
    "    configlist_RIDGE.append(conf)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalua_config(configlist_LASSO, X_train_tran, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalua_config(configlist_RIDGE, X_train_tran, y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función que genera un data frame que muestra para cada condiguración y partición del CV el Error Cuadrático Medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabla_ecm_particiones(configs, X_train_data, y_train_data, k):\n",
    "    \"\"\"\n",
    "    El objetivo es evaluar con cross validation distintas configuraciones de hiperparámetros \n",
    "    que definen a un modelo y devolver un data frame que muestre para cada configuración el error cuadratico medio\n",
    "    para cada una de las particiones.\n",
    "    \n",
    "    Argumentos:\n",
    "    - configs (list): las distintas configuraciones a evaluar.\n",
    "    - X_train_data: conjunto de entrenamiento de los predictores\n",
    "    - y_train_data: conjunto de entrenamiento de la variable dependiente\n",
    "    - k: numero de folds para cross validation.\n",
    "    \n",
    "    El output es un data frame donde las columnas son el hiperparámetro lamda, la partición y el error cuadratico medio.\n",
    "    \"\"\"\n",
    "    ecms = pd.DataFrame(columns=[\"lamda\", \"particion\", \"ecm\"])\n",
    "    \n",
    "    for config in configs:\n",
    "        model = LogisticRegression(max_iter=1000, **config)\n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "        for i, (train_index, test_index) in enumerate(kf.split(X_train_data), 1):\n",
    "            X_train, X_test = X_train_data.iloc[train_index], X_train_data.iloc[test_index]\n",
    "            y_train, y_test = y_train_data[train_index], y_train_data[test_index]\n",
    "            metrics = evalua_metodo(model, X_train, y_train, X_test, y_test)\n",
    "            ecm= metrics.loc[metrics.index[0], \"ecm\"]\n",
    "            ecms = ecms.append({\"lamda\": 1/config[\"C\"], \"particion\": i, \"ecm\": ecm}, ignore_index=True)  \n",
    "            \n",
    "        \n",
    "    return ecms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Usamos la función para obtener la tabla para las configuraciones evaluadas previamente:\n",
    "ecms_RIDGE=tabla_ecm_particiones(configlist_RIDGE, X_train_tran, y_train, 10)\n",
    "ecms_RIDGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de LASSO, generamos una función que además de devolver el Error Cuadrático Medio, devuelva también la proporción de coeficientes que toman valor 0 para cada configuración de hiperparámetros y para cada partición del CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabla_ecm_coeficientes_lasso(configs, X_train_data, y_train_data, k):\n",
    "    \"\"\"\n",
    "    El objetivo es evaluar con cross validation distintas configuraciones de hiperparámetros \n",
    "    que definen a un modelo de Regresion Logistica con LASSO y devolver un data frame que muestre \n",
    "    para cada una de las particiones de cada configuración el error cuadratico medio y la proporcion \n",
    "    de variables para las que el coeficiente es 0.\n",
    "    \n",
    "    Argumentos:\n",
    "    - configs (list): las distintas configuraciones a evaluar.\n",
    "    - X_train_data: conjunto de entrenamiento de los predictores\n",
    "    - y_train_data: conjunto de entrenamiento de la variable dependiente\n",
    "    - k: numero de folds para cross validation.\n",
    "    \n",
    "    El output es un data frame donde las columnas son el hiperparámetro lamda, la partición, el error cuadratico medio\n",
    "    y la proporcion de variables para las que el coeficiente es 0.\n",
    "    \"\"\"\n",
    "    ecms = pd.DataFrame(columns=[\"lamda\", \"particion\", \"ecm\", \"proporcion_0\"])\n",
    "    \n",
    "    for config in configs:\n",
    "        model = LogisticRegression(max_iter=1000, **config)\n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "        for i, (train_index, test_index) in enumerate(kf.split(X_train_data), 1):\n",
    "            X_train, X_test = X_train_data.iloc[train_index], X_train_data.iloc[test_index]\n",
    "            y_train, y_test = y_train_data[train_index], y_train_data[test_index]\n",
    "            metrics = evalua_metodo(model, X_train, y_train, X_test, y_test)\n",
    "            ecm= metrics.loc[metrics.index[0], \"ecm\"]\n",
    "            \n",
    "            \n",
    "            modelfit=model.fit(X_train, y_train)\n",
    "            coeficientes_finales = pd.DataFrame([np.array(X_train.columns.tolist()), modelfit.coef_[0]]).T\n",
    "            coeficientes_finales.columns = ['feature','coeficiente']\n",
    "\n",
    "            coef_0 = (coeficientes_finales['coeficiente'] == 0).sum()\n",
    "            proporcion_0= coef_0 / len(coeficientes_finales)\n",
    "            ecms = ecms.append({\"lamda\": 1/config[\"C\"], \"particion\": i, \"ecm\": ecm, \"proporcion_0\": proporcion_0}, ignore_index=True)  \n",
    "            \n",
    "        \n",
    "    return ecms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecms_LASSO=tabla_ecm_coeficientes_lasso(configlist_LASSO, X_train_tran, y_train, 10)\n",
    "ecms_LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#Graficamos el Boxplot para Ridge:\n",
    "box_RIDGE = sns.boxplot(data=ecms_RIDGE, x=\"lamda\", y=\"ecm\")\n",
    "box_RIDGE.set_xticklabels(['1e-0.5', '0.0001', '0.001', \"0.01\", \"0.1\", \"1\", \"10\", \"100\", \"1000\", \"10000\", \"1e+0.5\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos el Boxplot para Lasso que muestra la distribución en el Error Cuadrático Medio para las que el coeficiente es 0 según el lamda\n",
    "\n",
    "box_LASSO = sns.boxplot(data=ecms_LASSO, x=\"lamda\", y=\"ecm\")\n",
    "box_LASSO.set_xticklabels(['1e-0.5', '0.0001', '0.001', \"0.01\", \"0.1\", \"1\", \"10\", \"100\", \"1000\", \"10000\", \"1e+0.5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos el Boxplot para Lasso que muestra la distribución en la proporción de variables para las que el coeficiente es 0 según el lamda\n",
    "\n",
    "box_LASSO_coeficientes = sns.boxplot(data=ecms_LASSO, x=\"lamda\", y=\"proporcion_0\")\n",
    "box_LASSO_coeficientes.set_xticklabels(['1e-0.5', '0.0001', '0.001', \"0.01\", \"0.1\", \"1\", \"10\", \"100\", \"1000\", \"10000\", \"1e+0.5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para el modelo con el valor óptimo de Lamda (10), generamos un dataframe que contenga los coeficientes asignados a cada variable, y dos dataframes que contienen los coeficientes que toman valor igual a 0 y distinto de 0.\n",
    "model = LogisticRegression(max_iter=1000, penalty=\"l1\", C=0.1, solver=\"saga\")\n",
    "model.fit(X_train_tran, y_train)\n",
    "coeficientes = pd.DataFrame([np.array(X_train_tran.columns.tolist()), model.coef_[0]]).T\n",
    "coeficientes.columns = ['feature','coeficiente']\n",
    "ceros=coeficientes.drop(coeficientes[coeficientes[\"coeficiente\"]!=0].index)\n",
    "noceros=coeficientes.drop(coeficientes[coeficientes[\"coeficiente\"]==0].index)\n",
    "coeficientes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.** A continuación, mostramos las variables que tienen un coeficiente de cero; es decir, las variables que fueron descartadas. A continuación, comentaremos algunas de las variables más llamativas y justificaremos la decisión de descartarlas:\n",
    "\n",
    "* `ESTADO_3 `: Esta variable es una dummy que indica que la persona es inactiva. La categoría base es estar ocupado. Podría tener sentido que una situación de inactividad no ayude a explicar si la persona es pobre o no, principalmente, porque esta población incluye a los jubilados o estudiantes. Aparentemente, tiene mayor relevancia estar desocupado (no inactivo) para predecir la pobreza.\n",
    "* `CAT_OCUP`s : Estas variables son dummies por cada tipo de ocupación. Nuestro modelo nos muestra que la condición de estar ocupado es necesaria para predecir la pobreza; sin embargo, el tipo de ocupación para no ser tan relevante para la predicción.\n",
    "* `CAT_INAC`s: Previamente mencionamos que la condición de inactividad quedó descartada como predictor. Entonces, los distintos tipos de inactividad también fueron descartados.\n",
    "* `PP02C1` - `PP02C8`: Son variables relacionadas a la forma de búsqueda de trabajo, como si es que tiene contactos, si consultó parientes, se anotó en bolsas de trabajo, contestó avisos en diarios/internet, etc. Estas variables fueron descartadas por el modelo. Es posible que el mecanismo por el cual las personas encuentran trabajo no es relevante para predecir la condición de pobreza. Lo que importa es tener o no trabajo, por lo que la variable `ESTADO_2` sí fue considerada por modelo, ya que muestra la diferencia entre los ocupados y desocupados.\n",
    "* `IV1_6` - `II9_2`: Son variables relacionadas a las características del hogar. Por ejemplo, tenemos el tipo de vivienda, usos que se le da a las habitaciones, material del techo, si tiene activos como cocina, lavadero, garage, etc. Esto nos pareció extraño, ya que esperaríamos que estas variables del hogar sean también relevantes para predecir la pobreza. Bajo el enfoque multidimensional de la pobreza, estas variables pertenecen también a una dimensión relevante. Sin embargo, algunas otras variables sobre características del hogar sí fueron aceptadas por el modelo. Por ejemplo, el número de habitaciones, si el agua se obtiene por cañería o fuera del terreno, o si es agua de red pública o perforación con bomba a motor. Esto nos da una idea de que no todas las características del hogar ayudan para predecir correctamente la pobreza, sino solamente aquellas que influyen -quizá- de manera más directa en la calidad de vida de las personas.\n",
    "* `VII1_1` - `VII2_4`: Las variables sobre cómo se organizan las tareas en el hogar no ayuda para predecir la pobreza.\n",
    "* `IX_MEN`: La cantidad de miembros en el hogar menores a 10 años también fue descartada.\n",
    "\n",
    "Si bien se descartan muchas variables que inicialmente creíamos relevantes para medir el bienestar de las personas, es importante recordar que en la EPH se mide la **pobreza monetaria**. Según el enfoque de pobreza multidimensional, pueden existir otras dimensiones relevantes para el desarrollo humano, como la educación o las condiciones del hogar, que no son abarcadas bajo el enfoque unidimensional monetario. Esto quiere decir, pueden existir *pobre invisibles* que no captura el enfoque monetario. Por tanto, como en esta investigación buscamos predecir la pobreza monetaria, algunas variables pueden ser descartadas; sin embargo, las mismas podrían ser relevantes para un enfoque multidimensional de la pobreza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cero_values = ceros['feature'].tolist()\n",
    "cero_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.**\n",
    "\n",
    "En el ejercicio 5 podemos ver que el desempeño de LASSO y RIDGE es similar. El Error Cuadrático Medio para el lamda óptimo en regresión logística con RIDGE es 0.2714 y con LASSO es 0.2711, por lo que son similares, si bien el ECM de LASSO es ligeramente menor. No obstante, el lamda óptimo varía entre ambos métodos de regularización (es 10 para LASSO y 100 para Ridge). Esto también puede observarse en los Box plots, que muestran que en el caso de Ridge el lamda=100 es el que genera el ECM promedio más bajo y con menos varianza, mientras que para el caso de LASSO los lamdas a partir de 100 comienzan a tener Errores Cuadrático Medio Mayores. Esto se debe, en parte, a que a partir de esa penalidad la proporción de coeficientes que toman valor 0 se acerca mucho a 1 y se vuelve igual a 1 para todo lamda a partir de 1000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.**\n",
    "\n",
    "Obtenemos las métricas de precisión para los dos modelos logit con lamda optimos, para el modelo de vecinos cercanos (con 3 y 5 vecinos), y para el modelo de Análisis Discriminante Lineal.\n",
    "\n",
    "Observamos que el método que predice mejor es el de Vecinos Cercanos con 3 vecinos, ya que tiene menor Error Cuadrático Medio, mayor área bajo la curva ROC y mayor Accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "configlist = [{'penalty': 'l1', 'C': 0.1, 'solver': 'saga'}, {'penalty': 'l2', 'C': 0.01}]\n",
    "\n",
    "vecinos_prueba= [3, 5]\n",
    "\n",
    "evalua_multiples_metodos(configlist, X_train_tran, y_train, X_test_tran, y_test, 5, vecinos_prueba, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.**\n",
    "\n",
    "Utilizamos el método de vecinos cercanos con 3 vecinos para predecir la pobreza en la base norespondieron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nr = norespondieron.drop(columns=[\"CODUSU\", \"NRO_HOGAR\"])\n",
    "X_nr[\"const\"]= 1 \n",
    "variables_X=X_train.columns.tolist()\n",
    "X_nr=X_nr[variables_X] #Hacemos que las variables en X_nr tengan el mismo orden que en la base que usamos para entrenar el modelo.\n",
    "\n",
    "#Notamos que la variable \"II3_1\" aparece duplicada, así que nos quedamos con 1 sola:\n",
    "column_values = X_nr.iloc[:, 115].values\n",
    "X_nr[\"prueba\"]=column_values\n",
    "column_values_train = X_train.iloc[:, 115].values\n",
    "X_train[\"prueba\"]=column_values_train\n",
    "X_nr=X_nr.drop(columns=[\"II3_1\"])\n",
    "X_train=X_train.drop(columns=[\"II3_1\"])\n",
    "X_nr = X_nr.rename(columns={'prueba': 'II3_1'})\n",
    "X_train = X_train.rename(columns={'prueba': 'II3_1'})\n",
    "\n",
    "#Usamos el modelo de vecinos cercanos con 3 vecinos para predecir la pobreza en la base norespondieron\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knnfit= knn.fit(X_train, y_train)\n",
    "y_pred_knn = knnfit.predict(X_nr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norespondieron['prediccion_pobreza'] = y_pred_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nos quedamos con una observación por hogar para calcular la tasa de pobreza:\n",
    "\n",
    "hogares = norespondieron.groupby(['CODUSU', 'NRO_HOGAR']).head(1)\n",
    "pobres_count=(hogares['prediccion_pobreza'] == 1).sum()\n",
    "nopobres_count=(hogares['prediccion_pobreza'] == 0).sum()\n",
    "total=nopobres_count + pobres_count\n",
    "\n",
    "print(\"El porcentaje de hogares pobres predicho en la muestra norespondieron es:\", pobres_count*100/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El porcentaje de pobreza predicho es alto (72,37%). Podría ocurrir que las personas que no reportan su ingreso, o que reportan que es 0 sea porque este es bajo, en cuyo caso sería esperable que la tasa de pobreza en esta submuestra sea mayor que para el subconjunto de hogares que sí reportaron ingresos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
